---
title: "Part 1: Foundational Excercises"
author: "Ken Trinh, Kesha Julien, David Corrales Garcia"
date: "2/27/2022"
output:
  pdf_document:
    toc: true
    number_sections: true
    toc_depth: 3
---
# Part 1: Foundational Exercises
## Professional Magic
Based on the information provided, we know that the two coins are represented by Bernoulli Random Variables. To determine whether or not it is indeed magic, we designed a test to flip the two coins three times and record the test statistics of the number of heads shown. In the case of a fair coin toss, our Null Hypothesis (Ho) is p = 1/2. Since we are flipping 2 coins three times, we would have at most 6 heads shown. So we plan to reject our Null Hypothesis if our test statistics is 0 or 6.

a. Here we want to calculate the type 1 error rate. Type 1 error rate refers to the probability of rejecting the Null Hypothesis (Ho) given the Null Hypothesis is True.
$$
  P(Rejecting\ H_o\ |\ H_o)
$$
Let the test statistics be $T = (X_1 + Y_1) + (X_2 + Y_2) + (X_3 + Y_3)$, $X_i$ and $Y_i$ takes on values [0, 1]. Taking the rejection regions as 0 and 6. We expand the type 1 error rate as follow:

$$
  P(Rejecting\ H_o\ |\ H_o) = P[(T = 0)\ U\ (T = 6)\ when\ T\ is \ Bernoulli(p/2)]
$$

Since each X, Y coin toss is independent:
$$
  P(Rejecting\ H_o\ |\ H_o) = P[(T = 0)] + P[(T = 6)]\ Independence
$$
Expanding the probability, and substituting $$p=1/2$$ into the system since we are given Ho, we apply multiplication for three coin tosses here since each (X, Y) pairs are IID, and we have three pairs.
$$
  P[(T = 0)] = [X1 = 0, Y1= 0, X2=0, Y2=0, X3=0, Y3=0] = (p/2)^3 = (1/4)^3 = (1/4)^3 = 1/64
$$

$$
  P[(T = 6)] = [X1 = 1, Y1= 1, X2=1, Y2=1, X3=1, Y3=1] = (p/2)^3 = (1/4)^3 = (1/4)^3 = 1/64
$$

$$
  P(Rejecting\ H_o\ |\ H_o) = 1/64 + 1/64 = 1/32 \approx 0.031
$$
b. Here we want to calculate the statistical power of our test for the alternative hypothesis. Statistical Power is related to Type 2 error rate and refers to the probability of supporting the Alternative Hypothesis (Ha) given that the Alternative Hypothesis is True, and the statistical power equals 1 - type 2 error rate.
$$
  P(Supporting\ H_a\ |\ H_a)
$$
We are given that Ha: $$p=3/4$$, and that our rejection region is [0, 6]:
$$
  P(number\ of\ heads\ in\ the\ rejection\ region\ |\ Ha) = P(number\ of\ heads\ in\ [0, 6]\ |\ Ha)
$$
Since each X, Y coin toss is independent:
$$
  P(Supporting\ H_a\ |\ H_a) = P[(T = 0)] + P[(T = 6)]\ Independence
$$
Expanding the probability, and substituting $$p=3/4$$ into the system since we are given Ha, we apply multiplication for 3 coin tosses here since each (X, Y) pairs are IID, and we have three pairs:
$$
  P[(T = 0)] = [X1 = 0, Y1= 0, X2=0, Y2=0, X3=0, Y3=0] = (p/2)^3 = (3/8)^3 = 27/512
$$

$$
  P[(T = 6)] = [X1 = 1, Y1= 1, X2=1, Y2=1, X3=1, Y3=1] = (p/2)^3 = (3/8)^3  = 27/512
$$
Thus, the statistical power, or the probability of not making a type 2 error is computed as follows:
$$
  Statistical\ Power = 27/512 + 27/512 = 27/256 \approx 0.105
$$

\newpage
## Wrong Test, Right Data

Let X be the Random Variable describing how much customers like the regular website. Let Y be the Random Variable showing how much customers like the mobile website. The scale of measurement is on the 5 points Likert scales. Given that the Likert scale is an ordinal scale, if we are using the paired t-test, we would violate the assumption that the t-test uses a metric scale. To calculate t-test statistics in the paired t-test, we know that we need to calculate an expected value and the sample variation for both random variable X, Y. For that, the scale has to be continuous. The Likert scale, being an ordinal scale, has no objective variation between one value to another. We believe this would cause one to compute a bias test statistics, likely larger, making our test sample to fall into the rejection region, which would increase the type 1 error rate. To correct for this, we suggest Wilcoxon Sign-Rank Test because the two samples are dependent.

\newpage
## Test Assumptions
### World Happiness
You would like to know whether people in countries with high GDP per capita (higher than the mean) are more happy or less happy than people in countries with low GDP (lower than the mean).
List all assumptions for a two-sample t-test. Then evaluate each assumption, presenting evidence based on your background knowledge, visualizations, and numerical summaries.
```{r}
# load the happiness dataset
happiness_dataset <- read.csv('datasets/happiness_WHR.csv')
```
Assumptions for a two-sample t-test:
1.  Metric scale.
```{r}
summary(happiness_dataset$Life.Ladder)
```
Even though the life ladder scale is ordinal in nature, the data presented could be considered metric as it includes decimal averages for every individual in a country.
For this reason, we believe that the assumption that we are using metric data is met.

2.  IID data.
Sample 1: Happiness in countries with high GDP per capita.
Sample 2: Happiness levels in countries with low GDP per capita.
Given the two samples, we believe that the happiness level of one individual would not provide information about the happiness of another individual, making us conclude that these values are independent. 
Provided that each individual's happiness level is derived from the same scale, we agreed that the values of the samples can also be considered identically distributed.
For these two reasons, we believe the assumptions of IID are met.

3.  Considering the sample size, we agree that there would be no major deviations from normality.

```{R}
summary(happiness_dataset$Log.GDP.per.capita)
happiness_dataset$is_high_gdp<-happiness_dataset$Log.GDP.per.capita>9.584
sum(!is.na(happiness_dataset$Life.Ladder[happiness_dataset$is_high_gdp==T]))
sum(!is.na(happiness_dataset$Life.Ladder[happiness_dataset$is_high_gdp==F]))
```
Assuming that the samples are split based on the GDP per capita mean value of 9.584, we found that both samples exceed size of 30, so the Central Limit Theorem is met, expecting approximation of a normal distribution.
For this reason, we conclude that there are no major deviations from normality.


### Legislators

```{R}
legislators_current <- read.csv('datasets/legislators-current.csv')
```
Assumptions for a Wilcoxon rank-sum test (Hypothesis of Comparison):

1.  Our data is an ordinal scale:
The original data was given as date of birth. To make the sample usable, the senator age is calculated using the current year 2022 minus the birth year, as a discrete value from 0 to 100. There is no continuous relationship between two age interval (ie. 89, 90 are two distinct ages). For that reason, we believe the assumption that the age is on an ordinal scale is met.
```{3}
# make a column for age and plot it
legislators_current$age <- 2022 - strtoi(substr(legislators_current$birthday, 1, 4))
hist(legislators_current$age, main = 'Senator Age', breaks=20)
```

2.  IID:
The age of the senator is independent because a birth year of one senator does not provide any information to determine the birth year of another senator. The senator age is also identically distributed because it is derived from the same birth year convention on a regular calendar scale. For those reasons, the IID assumption is met!


### Wine and health

```{r}
# Installing pre-req pacakges
# load the wine dataset
install.packages("wooldridge") 
library(wooldridge) 
```
The wine dataset contains observations of variables related to wine consumption for 21 countries. We would like to use this data to test whether countries have more deaths from heart disease or from liver disease.

Assumptions for a Wilcoxon Signed-Rank Test:

1) The data is a metric scale.
  - In particular, X and Y are both measured on the same metric scale.

By viewing the format of the wine dataset, the columns heart and liver are denoted as heart disease deaths per 100,000 and liver disease deaths per 100,000, respectively. Since the two columns both measure death per 100,000 individuals, it is measuring a continuous average number of death and is measuring from the same scale. Therefore the assumption that both samples are metric is met. 
```{r}
summary(wine$heart)
summary(wine$liver)
```


2) IID data.
  - In particular, each pair $(X_i,Y_i)$ is drawn from the same distribution, independently of all other pairs.
  
Let $X_i$ be the number of deaths caused by heart disease in country i and $Y_i$ be the number of deaths caused by liver disease in the same country i, such that, both, $X_i$ and $Y_i$ are pairs of the same total distribution of death of the same country, making the pair naturally dependent by virtue of being part of the same country, and independent from other pairs, belonging to other countries. In this way, we find that we can generate pairs ($X_i$,$Y_i$) from the same distribution and independently of other pairs, thus, meeting this assumption as well. This can be seen in the following distribution \@ref(plot:histogram-dist)

```{r histogram-dist, fig.cap='Distribution of Heart and Liver Disease Related Death', fig.pos='!b', fig.width=12, fig.height = 4}
par(mfrow=c(1,2)) 
hist(wine$heart, xlab="Heart Disease Related Death", main="Heart Disease Related Death")
hist(wine$liver, xlab="Liver Disease Related Death", main="Liver Disease Related Death")
```

3) The distribution of the difference $X - Y$ is symmetric around some mean $\mu$

Here the number of observations for the differences between death by heart disease and death by liver disease is calculated to be n=21. This number is not large enough to meet the 30 minimum as ascribed by the the Central Limit Theorem. Additionally, when plotted in a histogram, we see that the distribution does not show a normal distribution either. For those reasons, we agree that the assumption that the distribution of the difference $X - Y$ is symmetric around some mean $\mu$ is not met.

```{r}
wine$diff <- wine$heart-wine$liver
sum(!is.na(wine$diff))
hist(wine$heart-wine$liver, 
     xlab="Differences", 
     main='Differences between Heart/Liver Disease Death', 
     breaks=5)
```


### Attitudes toward the religious
```{r}
GSS_religion <- read.csv('datasets/GSS_religion.csv')
head(GSS_religion)
```
Assumptions for a Paired t-Test:

1) Metric scale.
  - In particular, the t-test is not valid for variables which only have an ordinal structure.
```{r}
summary(GSS_religion$prottemp)
summary(GSS_religion$cathtemp)
```
To assess whether the two protemp and cathtemp samples are on a metric scale, we evaluated if the data is presented with a degree of difference between categories. From the description of the feeling thermometer (a 0-100 rating), a rating between 50 degrees and 100 degrees means that one feels favorable and warm toward the group and a rating between 0 degrees and 50 degrees means that one does not feel favorable toward the group. Theoretically, if one looks at the interval [49,51] one can see that any values between 50 to 51 degrees fall into the favorable group while any values between 49 to 50 fall into the unfavorable group. Using this logic, we find that there is information between interval for these two samples. Therefore, the assumption of metric scale is met.

2) IID data.
 - In particular, each pair of measurements $(X_i, Y_i)$ is drawn from the same distribution, independently of all other pairs.

The description of the GSS questions suggested that the feeling thermometer is a rating from 0 to 100. The rating is obtain by asking the participant to give a rating. Keeping this in mind when viewing the summary of the two samples in this dataset, prottemp and cathtemp, we see that both have a minimum value of 0 and a maximum value of 100. Additionally, the inner quartile range, computed by taking the difference between the 3rd quartile and the 1st quartile, is equal to 35 for both samples. From this standpoint, each sample pair (X_i, Y_i) is drawn from the same distribution ranging from a scale of 0 to 100 shown in Figure \@ref(fig:box-plot). Therefore they are identically distributed. In terms of independence, because a given sample pair (X_i, Y_i) is an individual's rating on a protestant group and catholic group, it provides no information on another user rating for the two groups, we conclude that each sample pair is independent. For these reason, we believe that the assumption of IID data is met.


```{r box-plot, fig.cap='Distribution of Protestant and Catholic Temperature', fig.pos='!b', fig.width=12, fig.height = 4}
par(mfrow=c(1,2)) 
b1 <- boxplot(GSS_religion$prottemp, main="Protestant Temperature")
b2 <- boxplot(GSS_religion$cathtemp, main="Catholic Temperature")
```


3) The distribution of the difference between measurements has no major deviations from normality, considering the sample size.
 - In particular, the t-test is invalid for highly skewed distributions when sample size is larger than 30. It may also be invalid for very highly skewed distributions at higher sample sizes.
 
```{r}
hist(GSS_religion$prottemp-GSS_religion$cathtemp, 
     xlab="Differences",
     main='Differences between Protemp and Cathtemp', breaks=10)
```
To assess if the difference between the two measurements has no major deviation from normality, we first need to compute the differences between the two samples and count the number of non-NAN values in a results column. Here we see that the sample size is n=802, which exceeds a minimum bound of n=30. For this reason, we agree that the Central Limit Theorem assumption is met, approximating a normal distribution. Viewing the histogram with number of bins(breaks) = 10, we also see that the sample is relatively distributed with center at approximately 0. For these reasons, the assumption of no deviation from normality is met.
